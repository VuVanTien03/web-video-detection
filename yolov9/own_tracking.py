import cv2
from models.common import DetectMultiBackend, AutoShape
import torch
from deep_sort_realtime.deepsort_tracker import DeepSort
import queue
import threading
import numpy as np
import time 
# import onnxruntime 
# ===== Khá»Ÿi táº¡o model =====
weights_path = r"D:\code\python\DataMining\yolov9\owncode\data\gelan_t.pt"
class_file = r"D:\code\python\DataMining\yolov9\owncode\data\classes.names"
weight_onnx = r"D:\code\python\DataMining\yolov9\owncode\data\weight.onnx"

device = torch.device("cpu")

model_backend = DetectMultiBackend(weights=weights_path, device=device)
model = AutoShape(model_backend)
model.eval()

# ===== Biáº¿n toÃ n cá»¥c =====
frame_queue = queue.Queue()
result_queue = queue.Queue()
process_every_n = 5
tracker = DeepSort(max_age=5)

with open(class_file, "r") as f:
    class_names = f.read().strip().split("\n")

colors = np.random.randint(0, 255, size=(len(class_names), 3), dtype=int)


# ===== YOLO xá»­ lÃ½ á»Ÿ luá»“ng riÃªng =====
def yolo_worker():
    while True:
        item = frame_queue.get()
        if item is None:
            break
        index, frame = item
        with torch.no_grad():
            result = model(frame)
        result_queue.put((index, result))


# ===== Má»Ÿ vÃ  chiáº¿u video =====
def open_video(video_path):
    cap = cv2.VideoCapture(video_path)
    result_dict = {}
    frame_index = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Gá»­i frame cho YOLO xá»­ lÃ½ náº¿u cáº§n
        if frame_index % process_every_n == 0:
            frame_queue.put((frame_index, frame.copy()))

        # Nháº­n káº¿t quáº£ YOLO
        while not result_queue.empty():
            i, result = result_queue.get()
            result_dict[i] = result

        # Váº½ káº¿t quáº£ náº¿u frame hiá»‡n táº¡i Ä‘Ã£ cÃ³ káº¿t quáº£
        if frame_index in result_dict:
            predictions = result_dict[frame_index].pred[0]

            detections = []
            for det in predictions:
                x1, y1, x2, y2, conf, class_id = det[:6]
                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
                class_id = int(class_id)

                # DeepSort yÃªu cáº§u: [x, y, w, h], confidence, class_id
                detections.append([[x1, y1, x2 - x1, y2 - y1], conf.item(), class_id])

            # Update tracker
            tracks = tracker.update_tracks(detections, frame=frame)

            # Váº½ káº¿t quáº£
            for track in tracks:
                if not track.is_confirmed():
                    continue
                track_id = track.track_id
                ltrb = track.to_ltrb()  # [left, top, right, bottom]
                x1, y1, x2, y2 = map(int, ltrb)
                class_id = track.get_det_class()
                color = colors[class_id].tolist()
                label = f"{class_names[class_id]} ID:{track_id}"
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # Hiá»ƒn thá»‹ frame
        cv2.imshow("Object Tracking", frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

        frame_index += 1

    cap.release()
    cv2.destroyAllWindows()
    frame_queue.put(None) 
def read_class_object():
    class_names = []
    with open(class_file , "rt" ) as f : 
        lines = f.readlines()
        class_names  = lines.strip().split("\n")
    return class_names

def new_process_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frame_index = 0 
    result = None
    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000/fps)
    while cap.isOpened():
        ret , frame = cap.read()
        if not ret : 
            break 
        if frame_index ==  0 or frame_index % 15 == 0 : 
            frame_queue.put((frame_index , frame.copy()))
        # láº¥y káº¿t quáº£ tá»« yolo worker 
        if  not result_queue.empty():
            i , result = result_queue.get()
           
        if result != None : 
            predicts = result.pred[0]
            detection = []
            for det in predicts : 
                x1 , y1 , x2 , y2 , conf , class_id = det[:6]
                x1 , y1 , x2 , y2 = map(int , [x1 , y1 , x2 , y2])
                class_id = int(class_id)
                if conf < 0.6: 
                    continue
                if class_id == 3 :
                    color = colors[3].tolist()
                    label = class_names[class_id]
                    conf = str(conf.item())
                    cv2.rectangle(frame , (x1 , y1) , (x2,y2) , color , 2)
                    cv2.putText(frame , label , (x1 , y1-10) , cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                    cv2.putText(frame , conf , (x1 , y1-25) , cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                # detection.append([[x1 , y1 , x2-x1 , y2-y1] , conf.item() , class_id])
            # tracks = tracker.update_tracks(detection , frame = frame)
            # # váº½ káº¿t quáº£ 
            # for track in tracks:
            #     if not track.is_confirmed():
            #         continue
            #     track_id = track.track_id
            #     ltrb = track.to_ltrb()  # [left, top, right, bottom]
            #     x1, y1, x2, y2 = map(int, ltrb)
            #     class_id = track.get_det_class()
            #     color = colors[class_id].tolist()
            #     label = f"{class_names[class_id]} ID:{track_id}"
            #     cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            #     cv2.putText(frame, label, (x1, y1 - 10),
            #                 cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        # show video 
        cv2.imshow("Object tracking" , frame) 
        if cv2.waitKey(delay) & 0xFF == ord("q"):
            break 
        frame_index +=1 
    cap.release()
    cv2.destroyAllWindows()
    frame_queue.put(None)
def show_video(video_path):
    cap = cv2.VideoCapture(video_path)

    fps = cap.get(cv2.CAP_PROP_FPS)
    delay = int(1000 / fps)  # thá»i gian chá» giá»¯a cÃ¡c frame (tÃ­nh báº±ng ms)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        cv2.imshow("Video", frame)
        if cv2.waitKey(delay) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()
def time_excute(file_path):
    cap = cv2.VideoCapture(file_path)

    while cap.isOpened():
        ret , frame = cap.read()
        if not ret : 
            break
        start = time.time()
        with torch.no_grad():
            result = model(frame)
        end = time.time()
        print(f"time per frame : {end-start}")
# ===== Cháº¡y chÆ°Æ¡ng trÃ¬nh =====
if __name__ == "__main__":
    threading.Thread(target=yolo_worker, daemon=True).start()
    video_path = r"D:\Downloads\Gym Fight over a Cable Machine ðŸ¤” ðŸ’ªðŸ¼ Letâ€™s see who Wins ðŸ† #fighting #gym #fighter #shorts #viral.mp4"
    # open_video(video_path)
    video_path2 = r"D:\Downloads\videoplayback.mp4"
    video_path3 = r"D:\Downloads\10 Fastest Finishes in UFC History ðŸ†.mp4"
    # new_process_video(video_path3)
    time_excute(video_path3)
    # show_video(video_path3)
    